{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tSBWKujupf_u","executionInfo":{"status":"ok","timestamp":1735457848322,"user_tz":-345,"elapsed":51901,"user":{"displayName":"Prayas Shrestha","userId":"11670392251336215244"}},"outputId":"64bc2daa-5983-4caa-f2a2-779f08daa2e7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"8XIXr-N2kqz3","executionInfo":{"status":"ok","timestamp":1735457865815,"user_tz":-345,"elapsed":1536,"user":{"displayName":"Prayas Shrestha","userId":"11670392251336215244"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"o0mEdKDtkqz4","executionInfo":{"status":"ok","timestamp":1735457867620,"user_tz":-345,"elapsed":4,"user":{"displayName":"Prayas Shrestha","userId":"11670392251336215244"}}},"outputs":[],"source":["# writing train test split function\n","def train_test_split(\n","    X: np.ndarray, y: np.ndarray, test_size: float = 0.3, random_seed: int = 42\n","):\n","    \"\"\"Separates the dataset into train and test sets\n","    Args:\n","        X (np.ndarray): Feature matrix\n","        y (np.ndarray): Label Vector\n","        test_size (float, optional):\n","          Proportion of dataset to include in the test split\n","          Defaults to 0.3.\n","        random_seed (int, optional):\n","          Seed for reproduceability\n","          Defaults to 41.\n","    Returns:\n","      X_train, X_test, y_train, y_test : np.ndarray\n","        Training and testing splits of features and target.\n","    \"\"\"\n","    np.random.seed(random_seed)\n","    indices = np.arange(X.shape[0])\n","    np.random.shuffle(indices)\n","    test_split_size = int(len(X) * test_size)\n","    test_indices = indices[:test_split_size]\n","    train_indices = indices[test_split_size:]\n","    X_train, X_test = X[train_indices], X[test_indices]\n","    y_train, y_test = y[train_indices], y[test_indices]\n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"bzWbHZyxkqz5","executionInfo":{"status":"ok","timestamp":1735457869380,"user_tz":-345,"elapsed":4,"user":{"displayName":"Prayas Shrestha","userId":"11670392251336215244"}}},"outputs":[],"source":["# Define the cost function\n","def cost_function(X, Y, W):\n","    \"\"\"\n","    Parameters:\n","      This function finds the Mean Square Error.\n","    Input parameters:\n","      X: Feature Matrix\n","      Y: Target Matrix\n","      W: Weight Matrix\n","    Output Parameters:\n","      cost: accumulated mean square error.\n","    \"\"\"\n","    m = len(Y)\n","    predictions = X.dot(W)\n","    errors = Y - predictions\n","    squared_errors = errors**2\n","    cost = np.mean(squared_errors)\n","    return cost"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WBcoIY-5kqz5","executionInfo":{"status":"ok","timestamp":1735457870946,"user_tz":-345,"elapsed":6,"user":{"displayName":"Prayas Shrestha","userId":"11670392251336215244"}},"outputId":"345edd1d-fa0f-418b-80ef-588f7fa05401"},"outputs":[{"output_type":"stream","name":"stdout","text":["Proceed Further\n"]}],"source":["# Test case\n","X_test = np.array([[1, 2], [3, 4], [5, 6]])\n","Y_test = np.array([3, 7, 11])\n","W_test = np.array([1, 1])\n","cost = cost_function(X_test, Y_test, W_test)\n","if cost == 0:\n","  print(\"Proceed Further\")\n","else:\n","  print(\"something went wrong: Reimplement a cost function\")\n","  print(\"Cost function output:\", cost_function(X_test, Y_test, W_test))"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"DLSEuTHOkqz5","executionInfo":{"status":"ok","timestamp":1735457871446,"user_tz":-345,"elapsed":5,"user":{"displayName":"Prayas Shrestha","userId":"11670392251336215244"}}},"outputs":[],"source":["def gradient_descent(X, Y, W, alpha, iterations):\n","    \"\"\"\n","    Perform gradient descent to optimize the parameters of a linear regression model.\n","    Parameters:\n","      X (numpy.ndarray): Feature matrix (m x n).\n","      Y (numpy.ndarray): Target vector (m x 1).\n","      W (numpy.ndarray): Initial guess for parameters (n x 1).\n","      alpha (float): Learning rate.\n","      iterations (int): Number of iterations for gradient descent.\n","    Returns:\n","      tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values.\n","      W_update (numpy.ndarray): Updated parameters (n x 1).\n","      cost_history (list): History of cost values over iterations.\n","    \"\"\"\n","    # Initialize cost history\n","    cost_history = [0] * iterations\n","    W_START = 0\n","    # Number of samples\n","    m = len(Y)\n","    for iteration in range(iterations):\n","        # Step 1: Hypothesis Values\n","        Y_pred = X.dot(W)\n","        # Step 2: Difference between Hypothesis and Actual Y\n","        loss = Y_pred - Y\n","        # Step 3: Gradient Calculation\n","        dw = (2 / m) * X.T.dot(loss)\n","        # Step 4: Updating Values of W using Gradient\n","        W_update = W_START - alpha * dw\n","        # Step 5: New Cost Value\n","        cost = cost_function(X, Y, W_update)\n","        cost_history[iteration] = cost\n","    return W_update, cost_history"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LPL47wIFkqz6","executionInfo":{"status":"ok","timestamp":1735457872732,"user_tz":-345,"elapsed":9,"user":{"displayName":"Prayas Shrestha","userId":"11670392251336215244"}},"outputId":"6f4d1959-826d-4ae6-c1b7-eb5801cddb7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Final Parameters: [-0.00321979 -0.00367639 -0.0026994 ]\n","Cost History: [0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753, 0.30907277044736753]\n"]}],"source":["# Generate random test data\n","np.random.seed(0) # For reproducibility\n","X = np.random.rand(100, 3) # 100 samples, 3 features\n","Y = np.random.rand(100)\n","W = np.random.rand(3) # Initial guess for parameters\n","# Set hyperparameters\n","alpha = 0.01\n","iterations = 1000\n","# Test the gradient_descent function\n","final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n","# Print the final parameters and cost history\n","print(\"Final Parameters:\", final_params)\n","print(\"Cost History:\", cost_history)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"tlC6P3Zfkqz6","executionInfo":{"status":"ok","timestamp":1735457874415,"user_tz":-345,"elapsed":5,"user":{"displayName":"Prayas Shrestha","userId":"11670392251336215244"}}},"outputs":[],"source":["# Model Evaluation - RMSE\n","def rmse(Y, Y_pred):\n","    \"\"\"\n","    This Function calculates the Root Mean Squres.\n","    Input Arguments:\n","      Y: Array of actual(Target) Dependent Varaibles.\n","      Y_pred: Array of predeicted Dependent Varaibles.\n","    Output Arguments:\n","      rmse: Root Mean Square.\n","    \"\"\"\n","    errors = Y - Y_pred\n","    squared_errors = errors**2\n","    mse = np.mean(squared_errors)\n","    rmse = np.sqrt(mse)\n","    return rmse"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Blje6QOXkqz6","executionInfo":{"status":"ok","timestamp":1735457876271,"user_tz":-345,"elapsed":4,"user":{"displayName":"Prayas Shrestha","userId":"11670392251336215244"}}},"outputs":[],"source":["# Model Evaluation - R2\n","def r2(Y, Y_pred):\n","    \"\"\"\n","    This Function calculates the R Squared Error.\n","    Input Arguments:\n","      Y: Array of actual(Target) Dependent Varaibles.\n","      Y_pred: Array of predeicted Dependent Varaibles.\n","    Output Arguments:\n","      rsquared: R Squared Error.\n","    \"\"\"\n","    mean_y = np.mean(Y)\n","    ss_tot = np.sum((Y - mean_y) ** 2)\n","    ss_res = np.sum((Y - Y_pred) ** 2)\n","    r2 = 1 - (ss_res / ss_tot)\n","    return r2"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LmzkMPIpkqz7","executionInfo":{"status":"ok","timestamp":1735457931449,"user_tz":-345,"elapsed":506,"user":{"displayName":"Prayas Shrestha","userId":"11670392251336215244"}},"outputId":"f4ecd66b-a68e-45ac-8266-334c22640170"},"outputs":[{"output_type":"stream","name":"stdout","text":["Final Weights: [0.09595665 0.10040398]\n","Cost History (First 10 iterations): [3203.9957537723512, 3203.9957537723512, 3203.9957537723512, 3203.9957537723512, 3203.9957537723512, 3203.9957537723512, 3203.9957537723512, 3203.9957537723512, 3203.9957537723512, 3203.9957537723512]\n","RMSE on Test Set: 56.541638421738384\n","R-Squared on Test Set: -11.771587151436643\n"]}],"source":["# Main Function\n","def main():\n","  # Step 1: Load the dataset\n","  data = pd.read_csv('/content/drive/MyDrive/student.csv')\n","\n","  # Step 2: Split the data into features (X) and target (Y)\n","  X = data[['Math', 'Reading']].values # Features: Math and Reading marks\n","  Y = data['Writing'].values # Target: Writing marks\n","\n","  # Step 3: Split the data into training and test sets (80% train, 20% test)\n","  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_seed=42)\n","\n","  # Step 4: Initialize weights (W) to zeros, learning rate and number of iterations\n","  W = np.zeros(X_train.shape[1]) # Initialize weights\n","  alpha = 0.00001 # Learning rate\n","  iterations = 1000 # Number of iterations for gradient descent\n","\n","  # Step 5: Perform Gradient Descent\n","  W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n","\n","  # Step 6: Make predictions on the test set\n","  Y_pred = np.dot(X_test, W_optimal)\n","\n","  # Step 7: Evaluate the model using RMSE and R-Squared\n","  model_rmse = rmse(Y_test, Y_pred)\n","  model_r2 = r2(Y_test, Y_pred)\n","\n","  # Step 8: Output the results\n","  print(\"Final Weights:\", W_optimal)\n","  print(\"Cost History (First 10 iterations):\", cost_history[:10])\n","  print(\"RMSE on Test Set:\", model_rmse)\n","  print(\"R-Squared on Test Set:\", model_r2)\n","\n","if __name__ == \"__main__\":\n","  main()"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}